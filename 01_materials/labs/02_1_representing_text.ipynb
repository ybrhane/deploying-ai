{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c602fe",
   "metadata": {},
   "source": [
    "# Representing Text\n",
    "\n",
    "We can represent text in many ways: character strings are a standard representation, but we can also create numerical representations of text. In this and the next few notebooks, we will explore and discuss a few of these representations to motivate our discussions of *embeddings*. Embeddings are a representation of text that will help us determine similarity between two blurbs (phrases, sentences, paragraphs, etc.) of text.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/02_text_representation_distance.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Image source: Mastering Text Similarity ([Guadagnolo, 2024](https://medium.com/eni-digitalks/mastering-text-similarity-combining-embedding-techniques-and-distance-metrics-98d3bb80b1b6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc6a28",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "The process of converting text into a numerical vectors is sometimes called vectorization. \n",
    "\n",
    "A simple form of vectorization is to count the number of words in a phrase. [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn helps achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "phrases = [\"cats are fun\", \"dogs are also fun\", \"ice cream is great\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(phrases)\n",
    "print(x.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c0fcd5",
   "metadata": {},
   "source": [
    "Adding column labels via a pandas data frame, it is easier to understand the operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7729e1",
   "metadata": {},
   "source": [
    "## Why does it matter?\n",
    "\n",
    "Using count vectorization, we can calculate the vectors' cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5120e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(x)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f492c7",
   "metadata": {},
   "source": [
    "The cosine similarity between two vectors is the dot product normalized by the norms of each vector (see, for example, [this discussion](https://nlp.stanford.edu/IR-book/html/htmledition/dot-products-1.html) and [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)). This means:\n",
    "\n",
    "+ The phrase `\"cats are fun\"` is represented by the vector `[0    1     1     0    1]`.\n",
    "+ The phrase `\"dogs are also fun\"` is represented by       `[1    1     0     1    1]`.\n",
    "+ The dot product of these vectors is the sum of the pair-wise product of their elements: `0*1 + 1*1 + 1*0 + 0*1 + 1*1 = 2`.\n",
    "+ The norm of each vector is the usual Eucledean norm: `sqrt(0^2 + 1^2 + 1^2 + 0^2  + 1^2)` and `sqrt(1^2 + 1^2 + 0^2 + 1^2  + 1^2)`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9fefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0*1 + 1*1 + 1*0 + 0*1 + 1*1\n",
    "x = np.sqrt(0**2 + 1**2 + 1**2 + 0**2  + 1**2)\n",
    "y = np.sqrt(1**2 + 1**2 + 0**2 + 1**2  + 1**2)\n",
    "d/(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3292b1",
   "metadata": {},
   "source": [
    "Using this simple method, we obtain a metric that will tend to 1 as the vectors are more similar to each other, while they will tend to 0 when they are more dissimilar. When using CountVectorizer, we give the same weight to each word, regardless of the relative importance in the corpus (the group of documents or phrases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422397d",
   "metadata": {},
   "source": [
    "# tf-idf Vectorization\n",
    "\n",
    "We can enhance the similarity metric by counting better: we want to give more importance to rarer words that are uncommon in the corpus. This way we can reduce the relative importance of very common works (ex., \"the\", \"a\", \"is\", etc.) which can carry little meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad430940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(phrases)\n",
    "print(x.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec89f5c",
   "metadata": {},
   "source": [
    "From [sklearn's documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting):\n",
    "\n",
    ">In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    ">\n",
    ">In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n",
    "\n",
    "tf-idf means the product of Term Frequency (tf) and Inverse Document Frequency (idf):\n",
    "\n",
    "+ Term frequency is the number of times that a token (a word in the example above) appears in a document.\n",
    "+ Inverse document frequency is given by \n",
    "\n",
    "$$\n",
    "idf(t) = log \\frac{1+n}{n+df(t)} +1.\n",
    "$$\n",
    "+ In the equation above, $n$ is the total number of documents, and $df(t)$ is the number of documents in the document set that contain the term $t$.\n",
    "+ The resulting tf-idf vectors are normalized by the norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(x)\n",
    "print(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
