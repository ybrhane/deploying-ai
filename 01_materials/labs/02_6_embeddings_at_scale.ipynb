{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0076589",
   "metadata": {},
   "source": [
    "# All of the Above, but More\n",
    "\n",
    "Conceptually, we have covered the fundamental concepts of embeddings and similarity calculations. These two concepts enable capabilities that are important in language tasks and that form the foundation of agentic capabilities ([OpenAI, 2025](https://platform.openai.com/docs/guides/embeddings)):\n",
    "\n",
    "- Search\n",
    "- Clustering\n",
    "- Recommendations\n",
    "- Anomaly detection\n",
    "- Diversity measurement\n",
    "- Classification\n",
    "\n",
    "## Document RAG\n",
    "\n",
    "Some of these tasks are related to Retrieval-Augmented Generation (RAG). In the diagram below, we depict how to split a document, each chunk's embeddings and store them in a vector DB.\n",
    "\n",
    "![](./img/02_document_rag_embed.png)\n",
    "\n",
    "Once embeddings are stored, given a query we can use proximity search to find the nearest chunk. The chunk (and other related data) are context in prompt sent to an LLM.\n",
    "\n",
    "![](./img/02_document_rag_query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66389a6",
   "metadata": {},
   "source": [
    "# Introducing LangChain\n",
    "\n",
    "[LangChain](https://www.langchain.com/) is a set of tools that support cross-model for agent engineering. The library is useful and popular among the many options available.\n",
    "\n",
    "Some useful resources are:\n",
    "\n",
    "- [LangChain Documentation](https://docs.langchain.com/).\n",
    "- [Directory of LangChain Resources](https://www.langchain.com/resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf8891",
   "metadata": {},
   "source": [
    "## Document Splitting \n",
    "\n",
    "Document splitting  or chunking is usually the first step in any RAG setup. The idea is that we want to split documents into smaller sections to:\n",
    "\n",
    "- Comply with the models context length constraints.\n",
    "- Enhance search quality.\n",
    "- Reduce latency.\n",
    "- Control costs.\n",
    "\n",
    "![](./img/02_document_rag_embed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693b7ec",
   "metadata": {},
   "source": [
    "LangChain contains a family of [document loaders](https://python.langchain.com/docs/integrations/document_loaders/). Each document loader has its own set of parameters, but they all implement the `.load()` method. A few examples include:\n",
    "\n",
    "### Common File Types\n",
    "\n",
    "- [CSVLoader](https://python.langchain.com/docs/integrations/document_loaders/csv): CSV files\n",
    "- [DirectoryLoader](https://python.langchain.com/docs/how_to/document_loader_directory): All files in a given directory.\n",
    "- [Unstructured](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file): Many file types (see https://docs.unstructured.io/platform/supported-file-types)\n",
    "- [JSONLoader](https://python.langchain.com/docs/integrations/document_loaders/json): JSON files\n",
    "\n",
    "### PDF\n",
    "\n",
    "- [PyPDF](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader): Uses - [pypdf](https://pypi.org/project/pypdf/) to load and parse PDFs\t(Package).\n",
    "- [Unstructured](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file): Uses [Unstructured's](https://pypi.org/project/unstructured/) open source library to load PDFs\t(Package).\n",
    "- [PDFPlumber](https://python.langchain.com/docs/integrations/document_loaders/pdfplumber):  Load PDF files using [PDFPlumber](https://pypi.org/project/pdfplumber/)\t(Package).\n",
    "\n",
    "\n",
    "### Web Pages\n",
    "\n",
    "- [Web](https://python.langchain.com/docs/integrations/document_loaders/web_base): Uses urllib and BeautifulSoup to load and parse HTML web pages\t(Package).\n",
    "- [Unstructured](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file): Uses [Unstructured](https://pypi.org/project/unstructured/) to load and parse web pages (Package).\n",
    "- [RecursiveURL](https://python.langchain.com/docs/integrations/document_loaders/recursive_url): Recursively scrapes all child links from a root URL (Package).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a184e",
   "metadata": {},
   "source": [
    "## JSONLoader\n",
    "\n",
    "[JSONLoader](https://python.langchain.com/docs/integrations/document_loaders/json/) implements a JSON (including JSON lines) document loader. JSONLoader uses [`jq`](https://jqlang.org/) to specify hwo to use the data passed in the document. \n",
    "\n",
    "A few notes on the code below:\n",
    "\n",
    "- `jq_schema=\".\"` indicates that we will read all keys from each JSON line. The [`jq specification`](https://jqlang.org/manual/#basic-filters) affords flexible filtering. \n",
    "- `content_key=\"content\"` is required when more than one key is included in `jq_schema`.\n",
    "- `json_lines=True` means that the file is a [JSON lines file](https://jsonlines.org/). Each line of a JSON line file is a fully compliant JSON.\n",
    "- `metadata_func=get_metadata` indicates that we want to use the function `get_metadata()` to extract metdata from the filtered JSON line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(record:dict, metadata: dict) -> dict:\n",
    "    metadata['reviewid'] = record.get('reviewid')\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\"../../05_src/documents/pitchfork_content.jsonl\", \n",
    "                    jq_schema=\".\",\n",
    "                    content_key=\"content\",\n",
    "                    json_lines=True,\n",
    "                    text_content=True,\n",
    "                    metadata_func=get_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86064ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1].to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a504886",
   "metadata": {},
   "source": [
    "## Splitting Documents\n",
    "\n",
    "There are good reasons to split documents. As explained in [LangChain's Documentation](https://python.langchain.com/docs/concepts/text_splitters/#why-split-documents):\n",
    "\n",
    "\n",
    "- Handling non-uniform document lengths: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.\n",
    "- Overcoming model limitations: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.\n",
    "- Improving representation quality: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.\n",
    "- Enhancing retrieval precision: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.\n",
    "- Optimizing computational resources: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.\n",
    "\n",
    "## Text Splitters in LangChain\n",
    "\n",
    "LangChain contains a family of [document splitters](https://docs.langchain.com/oss/python/integrations/splitters/index):\n",
    "\n",
    "- Length-based: simple and intuitive approach that ensures a specific text length. Can be based on [characters](https://python.langchain.com/docs/how_to/character_text_splitter/) or [tokens](https://python.langchain.com/docs/how_to/split_by_token/).\n",
    "- Text structure-based: tries to use the natural structure of text, including paragraphs, sentences, and words. More specifically:\n",
    "\n",
    "    + The [RecursiveCharacterTextSplitter](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter) attempts to keep larger units (e.g., paragraphs) intact.\n",
    "    + If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n",
    "    + This process continues down to the word level if necessary.\n",
    "\n",
    "- Document Structure-based: Uses the structure of documents in specific formats, including Markdown, HTML, and JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ee835",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000, \n",
    "    chunk_overlap=200, \n",
    "    length_function = len, \n",
    "    add_start_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(data)\n",
    "print(f'Split {len(data)} reviews (documents) into {len(chunks)} chunks.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d48f85",
   "metadata": {},
   "source": [
    "Notice that the output documents (the \"chunks\") include the keys:\n",
    "\n",
    "- `seq_num`: a sequential number identifying each of the original documents. \n",
    "- `start_index`: the starting index for the chunk.\n",
    "- `page_content`: text of the document chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfba0b",
   "metadata": {},
   "source": [
    "## Batch Embeddings\n",
    "\n",
    "We now have a large number of documents for which we need embeddings. We could use a direct call to the Embeddings API. However, here we demonstrate how to request embeddings using the [Batch API](https://platform.openai.com/docs/api-reference/batch). From the documentation:\n",
    "\n",
    "The Batch API is used to send asynchronous groups of requests. This API offers lower costs, a separate pool of significantly higher rate limits, and a clear 24-hour turnaround time. The service is ideal for processing jobs that don't require immediate responses. \n",
    "\n",
    "A couple of useful references are: \n",
    "\n",
    "- [Batch API Guide](https://platform.openai.com/docs/guides/batch)\n",
    "- [API Reference](https://platform.openai.com/docs/api-reference/batch)\n",
    "\n",
    "## Creating Batches\n",
    "\n",
    "The batch process works as follows:\n",
    "\n",
    "1. Prepare the batch file. Batches start with a .jsonl file where each line contains the details of an individual request to the API.\n",
    "2. Upload the batch file to input. We must first input the batch file, so that we can reference it below.\n",
    "3. Create the batch.    \n",
    "4. Check status of the batch.\n",
    "5. Retrieve the results.\n",
    "\n",
    "In addition to the steps above, the API allows us to list all batches and to cancel a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82d6de",
   "metadata": {},
   "source": [
    "### 1. Prepare the Batch File\n",
    "\n",
    "Batch processing using the API requires input files to follow a specific format. \n",
    "\n",
    "A few notes from the [documentation](https://platform.openai.com/docs/guides/batch#1-prepare-your-batch-file)\n",
    "\n",
    "+ Batches start with a .jsonl file where each line contains the details of an individual request to the API. \n",
    "+ The available endpoints are:\n",
    "\n",
    "    - Responses API: /v1/responses\n",
    "    - Chat Completions API: /v1/chat/completions \n",
    "    - Embeddings API: /v1/embeddings \n",
    "    - Completions API: /v1/completions \n",
    "    - Moderations API: /v1/moderations \n",
    "\n",
    "+ For a given input file, the parameters in each line's body field are the same as the parameters for the underlying endpoint. \n",
    "+ Each request **must include a unique custom_id value**, which you can use to reference results after completion. \n",
    "\n",
    "#### Rate Limits\n",
    "\n",
    "It is important to keep in mind the [API's rate limits](https://platform.openai.com/docs/guides/batch#rate-limits):\n",
    "\n",
    "\n",
    "+ **Per-batch limits**: A single batch may include up to 50,000 requests, and a batch input file can be up to 200 MB in size. Note that /v1/embeddings batches are also restricted to a maximum of 50,000 embedding inputs across all requests in the batch.\n",
    "+ **Enqueued prompt tokens per model**: Each model has a maximum number of enqueued prompt tokens allowed for batch processing. You can find these limits on the [Platform Settings](https://platform.openai.com/settings/organization/limits) page.\n",
    "\n",
    "It is important to note: \n",
    "\n",
    "> There are no limits for output tokens or number of submitted requests for the Batch API today. Because Batch API rate limits are a new, separate pool, using the Batch API will not consume tokens from your standard per-model rate limits, thereby offering you a convenient way to increase the number of requests and processed tokens you can use when querying our API \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd6f1c",
   "metadata": {},
   "source": [
    "We must create files that contain the `page_content` and an identifier that would arguably include important metadata (like 'reviewid' and a chunk identifier) of our document chunks. We also want to create files that are within the rate limits (i.e., at most 50,000 documents per batch).\n",
    "\n",
    "The batch definition jsonl should contain one line per request. [Each request is defined as](https://cookbook.openai.com/examples/batch_processing#creating-the-batch-file):\n",
    "\n",
    "```\n",
    "{\n",
    "    \"custom_id\": <REQUEST_ID>,\n",
    "    \"method\": \"POST\",\n",
    "    \"url\": \"/v1/chat/completions\",\n",
    "    \"body\": {\n",
    "        \"model\": <MODEL>,\n",
    "        \"messages\": <MESSAGES>,\n",
    "        // other parameters\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].metadata['reviewid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ccc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def prep_batch_file_for_embedding(input:list, output_path:str, max_lines_per_file:int=10000):\n",
    "    total_lines = len(input)\n",
    "    num_files = (total_lines // max_lines_per_file) + 1\n",
    "    print(f'Total lines: {total_lines}, Number of files to create: {num_files}')\n",
    "\n",
    "    for num_file in range(num_files):\n",
    "        start_index = num_file * max_lines_per_file\n",
    "        end_index = min(start_index + max_lines_per_file, total_lines)\n",
    "        output_file = os.path.join(output_path, f\"pitchfork_reviews_batch_{num_file+1}.jsonl\")\n",
    "        print(f'Creating file: {output_file} with lines from {start_index} to {end_index-1}')\n",
    "        create_single_batch_file(input, start_index, end_index, output_file)\n",
    "\n",
    "def create_single_batch_file(input, start_index, end_index, output_file):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for line in input[start_index:end_index]:\n",
    "            custom_id = (\n",
    "                    str(line.metadata['reviewid']) + \"_\" + \n",
    "                    str(line.metadata['seq_num']) + \"_\" + \n",
    "                    str(line.metadata['start_index'])\n",
    "                )\n",
    "            content = line.page_content\n",
    "            out_dict = {\n",
    "                    \"custom_id\": custom_id, \n",
    "                    \"method\": \"POST\", \n",
    "                    \"url\": \"/v1/embeddings\", \n",
    "                    \"body\": {\n",
    "                        \"model\": \"text-embedding-3-small\", \n",
    "                        \"input\": content\n",
    "                    }\n",
    "                }\n",
    "            outfile.write(json.dumps(out_dict) + '\\n')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dde9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_batch_file_for_embedding(\n",
    "    input=chunks, \n",
    "    output_path='../../05_src/documents/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedeaa62",
   "metadata": {},
   "source": [
    "### 2. Upload the Input File\n",
    "\n",
    "Before running the batch process, we will upload the files to the API. File management has some useful functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8545937",
   "metadata": {},
   "source": [
    "#### List available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "files = client.files.list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.to_dict()['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c63a2",
   "metadata": {},
   "source": [
    "#### Remove Files\n",
    "\n",
    "You can remove files from storage using code like the one below, which deletes all files in the account. \n",
    "Note: this is a destructive action that cannot be undone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa17f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files.to_dict()['data']:\n",
    "    print(f'Deleting file: {file[\"filename\"]}')\n",
    "    resp = client.files.delete(file[\"id\"])\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e271c",
   "metadata": {},
   "source": [
    "#### Search and Upload Files\n",
    "\n",
    "We search for the files that we created and upload them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b24fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "batch_files = glob('../../05_src/documents/pitchfork_reviews_batch_*.jsonl')\n",
    "batch_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "for b_file in tqdm(batch_files):\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(b_file, \"rb\"), \n",
    "        purpose='batch'\n",
    "    )\n",
    "    print(batch_input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba0e89",
   "metadata": {},
   "source": [
    "### 3. Create Batches\n",
    "\n",
    "As before, we can consult the files that we have in store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_files = client.files.list().to_dict()\n",
    "batch_file_ids = [file['id'] for file in batch_files['data']]\n",
    "batch_file_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b8366",
   "metadata": {},
   "source": [
    "At a difference with the files API, there is no easy way of removing batches that have a completed or failed state, so the description and status are important. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17762ef1",
   "metadata": {},
   "source": [
    "Now we can create the batch procedure. For each file, we create the batch with the call below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "batch_description = \"Pitchfork reviews content embeddings \" + timestamp \n",
    "\n",
    "for file_id in tqdm(batch_file_ids):\n",
    "    client.batches.create(\n",
    "            input_file_id = file_id,\n",
    "            endpoint=\"/v1/embeddings\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "                \"description\": batch_description,\n",
    "                \"timestamp\": timestamp\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_processes = client.batches.list().to_dict()\n",
    "batch_info= [\n",
    "    {'batch_id': batch['id'],\n",
    "     'description': batch['metadata']['description'],\n",
    "    'status': batch['status'],\n",
    "    'request_counts': batch['request_counts'],\n",
    "    'output_file_id': batch['output_file_id']}  \n",
    "            for batch in batch_processes['data'] if batch['metadata']['description'] == batch_description\n",
    "    ]\n",
    "batch_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305a373",
   "metadata": {},
   "source": [
    "If you need to cancel a batch, you can use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in batch_info:\n",
    "#     client.batches.cancel(batch['batch_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
