{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac9fa3b",
   "metadata": {},
   "source": [
    "# Representing Text\n",
    "\n",
    "We can represent text in many ways: character strings are a standard representation, but we can also create numerical representations of text. In this notebook we will discuss embeddings.\n",
    "\n",
    "## Features\n",
    "\n",
    "Features to any (machine learning) model can be continuous or categorical.\n",
    "\n",
    "- We use continuous features to represent numerical values: income, number of times the user clicked on a link, prices, etc.\n",
    "- Categorical features represent an instance of a class or category. They have a finite number of possible values: job title, genre of a movie, breed of a dog, etc.\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "An **embedding** is a trained numerical representation of a categorical feature:\n",
    "\n",
    "- We use the word *trained* to highlight that embeddings are learned during model training.\n",
    "- Different models and training procedures can be used to obtained embeddings. Word2Vec and BERT embeddings, for example, are different and capture different characteristics of the features.\n",
    "\n",
    "[OpenAI's documentation](https://platform.openai.com/docs/guides/embeddings) include a few uses of embeddings:\n",
    "\n",
    "\n",
    "- Search: results are ranked by relevance to a query string.\n",
    "- Clustering:  text strings are grouped by similarity.\n",
    "- Recommendations:  items with related text strings are recommended.\n",
    "- Anomaly detection:  outliers with little relatedness are identified.\n",
    "- Diversity measurement: similarity distributions are analyze.\n",
    "- Classification: text strings are classified by their most similar label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f396f4",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Embedding computations start with tokenization: representing the original text as tokens in a vocabulary. \n",
    "\n",
    "To illustrate the process, we can use the [`transformers`](https://huggingface.co/docs/transformers/en/index) library from [HuggingFace](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0957363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8870, 2024, 4569, 102]], 'token_type_ids': [[0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "documents = [\"cats are fun\"]\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased')\n",
    "tokens = tokenizer(documents)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e29323",
   "metadata": {},
   "source": [
    "In the code snippet above, we used the `transformers` library to obtain the tokens that represent the phrase 'cats are fun'. The tokenizer returns a dictionary with an entry called `'input_ids'`, which contains an array of four integers. These integers are the positions of each token in the model's vocabulary. The vocabulary is [`'bert-base-uncased'`](https://huggingface.co/bert-base-uncased/blob/main/vocab.txt) and it is applied using the method `.from_pretrained()`.\n",
    "\n",
    "We can show the vocabulary entries with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70b566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of 'cats': 8870\n",
      "Index of 'are': 2024\n",
      "Index of 'fun': 4569\n"
     ]
    }
   ],
   "source": [
    "print(f\"Index of 'cats': {tokenizer.vocab['cats']}\")\n",
    "print(f\"Index of 'are': {tokenizer.vocab['are']}\")\n",
    "print(f\"Index of 'fun': {tokenizer.vocab['fun']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2161fa",
   "metadata": {},
   "source": [
    "IDs 101 and 102 are special tokens:\n",
    "\n",
    "- ID 101 is the `[CLS]` token, indicating the begginning of a sequence.\n",
    "- ID 102 is the `[SEP]` token, indicating the end of a sequence.\n",
    "\n",
    "They are inserted automatically to the output of the BERT tokenizer. The BERT Tokenizer includes 30,522 unique tokens. In addition, the BERT tokenizer handles unkown tokens, `[UNK]`, using techniques such as WordPiece. You can read more about this tokenizer in [BERT Tokenization (Nowak, 2023)](https://tinkerd.net/blog/machine-learning/bert-tokenization/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8c5e5",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "The tokens obtained from the previous step are mapped to the model's precomputed embeddings. For each token in the model vocabulary, there is an embedding vector.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/02_embeddings.png\" width=\"700\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa04972f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
       "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
       "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
       "        ...,\n",
       "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
       "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
       "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "embedding_layer = model.embeddings\n",
    "embedding_layer.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377eac68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bdc2f8",
   "metadata": {},
   "source": [
    "The attribute `.weight` of the embedding layer shows the actual embeddings. It is a matrix of 30,522 rows and 768 columns (the object, in reality, is a 2-dimenional vector). \n",
    "\n",
    "+ The number of rows is equal to the size of the model vocabulary.\n",
    "+ The number of columns is the hidden size or the size of the model's internal representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb201c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8870, 2024, 4569, 102]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(['cats are fun'])\n",
    "input_ids = tokens.input_ids[0]\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee568e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
       "        [-0.0590, -0.0339,  0.0108,  ..., -0.0328, -0.0285,  0.0624],\n",
       "        [-0.0134, -0.0135,  0.0250,  ...,  0.0013, -0.0183,  0.0227],\n",
       "        [-0.0073, -0.0459,  0.0314,  ..., -0.0196, -0.0372, -0.0150],\n",
       "        [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_embeddings = embedding_layer.word_embeddings.weight[input_ids]\n",
    "doc_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d340a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "else:\n",
    "    client = OpenAI(api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab09379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6890865",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    # Freedom\n",
    "    \"Freedom consists not in doing what we like, but in having the right to do what we ought.\",\n",
    "    \"Those who deny freedom to others deserve it not for themselves.\",\n",
    "    \"Liberty, when it begins to take root, is a plant of rapid growth.\",\n",
    "    \"Freedom lies in being bold.\",\n",
    "    \"Is freedom anything else than the right to live as we wish?\",\n",
    "    \"I am no bird and no net ensnares me: I am a free human being with an independent will.\",\n",
    "    \"The secret to happiness is freedom... And the secret to freedom is courage.\"\n",
    "    \"Freedom is the oxygen of the soul.\", \n",
    "    \"Life without liberty is like a body without spirit.\"\n",
    "    # Friendship\n",
    "    \"There is nothing on this earth more to be prized than true friendship.\",\n",
    "    \"There are no strangers here; Only friends you havenâ€™t yet met.\",\n",
    "    \"Friendship is the only cement that will ever hold the world together.\",\n",
    "    \"A true friend is someone who is there for you when he'd rather be anywhere else.\",\n",
    "    \"Friendship is the golden thread that ties the heart of all the world.\", \n",
    "    \"Your friend is the man who knows all about you and still likes you.\",\n",
    "    \"A single rose can be my garden... a single friend, my world.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96df878",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [get_embedding(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.array(embeddings)\n",
    "embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775395af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"]).assign(lables = documents)\n",
    "ax = df.plot(kind='scatter', x='x', y='y', figsize=(8, 6))\n",
    "for i, row in df.iterrows():\n",
    "    ax.text(row['x'], row['y'], row['lables'], fontsize=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
